{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/20 09:02:28 WARN Utils: Your hostname, Abdiels-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.96 instead (on interface en0)\n",
      "22/05/20 09:02:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/homebrew/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/20 09:02:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "## Data Acquisition\n",
    "These exercises should go in a notebook or script named wrangle. Add, commit, and push your changes.\n",
    "\n",
    "This exercises uses the case.csv, dept.csv, and source.csv files from the san antonio 311 call dataset.\n",
    "\n",
    "1. Read the case, department, and source data into their own spark dataframes.\n",
    "\n",
    "2. Let's see how writing to the local disk works in spark:\n",
    "\n",
    "    - Write the code necessary to store the source data in both csv and json format, store these as sources_csv and sources_json\n",
    "    - Inspect your folder structure. What do you notice?\n",
    "\n",
    "3. Inspect the data in your dataframes. Are the data types appropriate? Write the code necessary to cast the values to the appropriate types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the case, department, and source data into their own spark dataframes.\n",
    "source_df = spark.read.csv('source.csv', sep= ',', header=True, inferSchema=True)\n",
    "case_df = spark.read.csv('case.csv', sep= ',', header=True, inferSchema=True)\n",
    "dept_df = spark.read.csv('dept.csv', sep= ',', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|source_id| source_username|\n",
      "+---------+----------------+\n",
      "|   100137|Merlene Blodgett|\n",
      "|   103582|     Carmen Cura|\n",
      "|   106463| Richard Sanchez|\n",
      "|   119403|  Betty De Hoyos|\n",
      "|   119555|  Socorro Quiara|\n",
      "+---------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------------+------------+---------+-------------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|   case_id|case_opened_date|case_closed_date|SLA_due_date|case_late|      num_days_late|case_closed|   dept_division|service_request_type|   SLA_days|case_status|source_id|     request_address|council_district|\n",
      "+----------+----------------+----------------+------------+---------+-------------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|1014127332|     1/1/18 0:42|    1/1/18 12:29|9/26/20 0:42|       NO| -998.5087616000001|        YES|Field Operations|        Stray Animal|      999.0|     Closed| svcCRMLS|2315  EL PASO ST,...|               5|\n",
      "|1014127333|     1/1/18 0:46|     1/3/18 8:11| 1/5/18 8:30|       NO|-2.0126041669999997|        YES|     Storm Water|Removal Of Obstru...|4.322222222|     Closed| svcCRMSS|2215  GOLIAD RD, ...|               3|\n",
      "|1014127334|     1/1/18 0:48|     1/2/18 7:57| 1/5/18 8:30|       NO|       -3.022337963|        YES|     Storm Water|Removal Of Obstru...|4.320729167|     Closed| svcCRMSS|102  PALFREY ST W...|               3|\n",
      "|1014127335|     1/1/18 1:29|     1/2/18 8:13|1/17/18 8:30|       NO|       -15.01148148|        YES|Code Enforcement|Front Or Side Yar...|16.29188657|     Closed| svcCRMSS|114  LA GARDE ST,...|               3|\n",
      "|1014127336|     1/1/18 1:34|    1/1/18 13:29| 1/1/18 4:34|      YES|0.37216435200000003|        YES|Field Operations|Animal Cruelty(Cr...|      0.125|     Closed| svcCRMSS|734  CLEARVIEW DR...|               7|\n",
      "+----------+----------------+----------------+------------+---------+-------------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "|       dept_division|           dept_name|standardized_dept_name|dept_subject_to_SLA|\n",
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "|     311 Call Center|    Customer Service|      Customer Service|                YES|\n",
      "|               Brush|Solid Waste Manag...|           Solid Waste|                YES|\n",
      "|     Clean and Green|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "|Clean and Green N...|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "|    Code Enforcement|Code Enforcement ...|  DSD/Code Enforcement|                YES|\n",
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how writing to the local disk works in spark:\n",
    "# Write the code necessary to store the source data in both csv and json format, store these as sources_csv and sources_json\n",
    "# Inspect your folder structure. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data in your dataframes. \n",
    "# Are the data types appropriate? Write the code necessary to cast the values to the appropriate types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. How old is the latest (in terms of days past SLA) currently open issue? How long has the oldest (in terms of days since opened) currently opened issue been open?\n",
    "2. How many Stray Animal cases are there?\n",
    "3. How many service requests that are assigned to the Field Operations department (dept_division) are not classified as \"Officer Standby\" request type (service_request_type)?\n",
    "\n",
    "4. Convert the council_district column to a string column.\n",
    "\n",
    "5. Extract the year from the case_closed_date column.\n",
    "6. Convert num_days_late from days to hours in new columns num_hours_late.\n",
    "\n",
    "7. Join the case data with the source and department data.\n",
    "\n",
    "8. Are there any cases that do not have a request source?\n",
    "\n",
    "9. What are the top 10 service request types in terms of number of requests?\n",
    "\n",
    "10. What are the top 10 service request types in terms of average days late?\n",
    "11. Does number of days late depend on department?\n",
    "12. How do number of days late depend on department and request type?\n",
    "\n",
    "- You might have noticed that the latest date in the dataset is fairly far off from the present day. To account for this, replace any occurances of the current time with the maximum date from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
